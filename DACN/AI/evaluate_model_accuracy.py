"""
Evaluate Model Accuracy - ƒê√°nh gi√° chi ti·∫øt ƒë·ªô ch√≠nh x√°c c·ªßa model
T√≠nh to√°n Accuracy, Precision, Recall, F1-Score v√† Confusion Matrix
"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support
import json

# Custom function cho L2 normalization
def l2_normalize_func(x):
    """L2 normalization function"""
    return tf.nn.l2_normalize(x, axis=1)

# ============================================================================
# CONFIGURATION
# ============================================================================

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, 'face_data')

# Ch·ªçn model ƒë·ªÉ evaluate (b·∫°n c√≥ th·ªÉ thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n n√†y)
MODEL_PATH = os.path.join(BASE_DIR, 'faceid_optimized_best.h5')
if not os.path.exists(MODEL_PATH):
    MODEL_PATH = os.path.join(BASE_DIR, 'faceid_model_tf_best.h5')
if not os.path.exists(MODEL_PATH):
    MODEL_PATH = os.path.join(BASE_DIR, 'faceid_model_tf.h5')

IMG_SIZE = (160, 160)
BATCH_SIZE = 16

print("=" * 80)
print("ƒê√ÅNH GI√Å ƒê·ªò CH√çNH X√ÅC MODEL FACEID")
print("=" * 80)
print(f"Model: {MODEL_PATH}")
print(f"Data: {DATA_DIR}")
print("=" * 80)

# ============================================================================
# LOAD MODEL
# ============================================================================

print("\n[1/6] Loading model...")
try:
    model = tf.keras.models.load_model(
        MODEL_PATH,
        custom_objects={'l2_normalize_func': l2_normalize_func}
    )
    print(f"‚úì Model loaded successfully")
    print(f"‚úì Input shape: {model.input_shape}")
    print(f"‚úì Output shape: {model.output_shape}")
except Exception as e:
    print(f"‚úó L·ªói khi load model: {e}")
    exit(1)

# ============================================================================
# PREPARE DATA
# ============================================================================

print("\n[2/6] Preparing validation data...")

# Validation data generator (kh√¥ng augmentation)
val_datagen = ImageDataGenerator(rescale=1./255)

val_generator = val_datagen.flow_from_directory(
    DATA_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False  # Kh√¥ng shuffle ƒë·ªÉ gi·ªØ nguy√™n th·ª© t·ª±
)

num_classes = val_generator.num_classes
class_names = list(val_generator.class_indices.keys())

print(f"‚úì S·ªë l∆∞·ª£ng ng∆∞·ªùi: {num_classes}")
print(f"‚úì T√™n c√°c ng∆∞·ªùi: {', '.join(class_names)}")
print(f"‚úì Validation samples: {val_generator.samples}")

# ============================================================================
# MAKE PREDICTIONS
# ============================================================================

print("\n[3/6] Making predictions...")

# Reset generator
val_generator.reset()

# Predict
predictions = model.predict(val_generator, verbose=1)
predicted_classes = np.argmax(predictions, axis=1)

# Get true labels
true_classes = val_generator.classes

print(f"‚úì Predictions completed")
print(f"‚úì Total predictions: {len(predicted_classes)}")

# ============================================================================
# CALCULATE METRICS
# ============================================================================

print("\n[4/6] Calculating metrics...")

# Overall accuracy
accuracy = accuracy_score(true_classes, predicted_classes)

# Precision, Recall, F1-Score (weighted average)
precision, recall, f1, support = precision_recall_fscore_support(
    true_classes, 
    predicted_classes, 
    average='weighted'
)

# Per-class metrics
precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(
    true_classes, 
    predicted_classes, 
    average=None,
    labels=range(num_classes)
)

print("\n" + "=" * 80)
print("K·∫æT QU·∫¢ T·ªîNG QU√ÅT")
print("=" * 80)
print(f"Overall Accuracy:  {accuracy * 100:.2f}%")
print(f"Weighted Precision: {precision * 100:.2f}%")
print(f"Weighted Recall:    {recall * 100:.2f}%")
print(f"Weighted F1-Score:  {f1 * 100:.2f}%")
print("=" * 80)

# ============================================================================
# DETAILED CLASSIFICATION REPORT
# ============================================================================

print("\n[5/6] Generating classification report...")

print("\n" + "=" * 80)
print("CHI TI·∫æT THEO T·ª™NG NG∆Ø·ªúI")
print("=" * 80)

report = classification_report(
    true_classes,
    predicted_classes,
    target_names=class_names,
    digits=4
)
print(report)

# ============================================================================
# CONFUSION MATRIX
# ============================================================================

print("\n[6/6] Creating confusion matrix...")

# Calculate confusion matrix
cm = confusion_matrix(true_classes, predicted_classes)

# Plot confusion matrix
plt.figure(figsize=(12, 10))
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=class_names,
    yticklabels=class_names,
    cbar_kws={'label': 'Count'}
)
plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()

# Save plot
cm_path = os.path.join(BASE_DIR, 'confusion_matrix.png')
plt.savefig(cm_path, dpi=150, bbox_inches='tight')
print(f"‚úì Confusion matrix saved: {cm_path}")

# ============================================================================
# PLOT PER-CLASS METRICS
# ============================================================================

# Plot per-class metrics
plt.figure(figsize=(14, 6))

x = np.arange(num_classes)
width = 0.25

plt.bar(x - width, precision_per_class * 100, width, label='Precision', color='skyblue')
plt.bar(x, recall_per_class * 100, width, label='Recall', color='lightcoral')
plt.bar(x + width, f1_per_class * 100, width, label='F1-Score', color='lightgreen')

plt.xlabel('Person', fontsize=12)
plt.ylabel('Score (%)', fontsize=12)
plt.title('Per-Class Metrics', fontsize=16, fontweight='bold')
plt.xticks(x, class_names, rotation=45, ha='right')
plt.ylim(0, 105)
plt.legend()
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()

metrics_path = os.path.join(BASE_DIR, 'per_class_metrics.png')
plt.savefig(metrics_path, dpi=150, bbox_inches='tight')
print(f"‚úì Per-class metrics saved: {metrics_path}")

# ============================================================================
# SAVE DETAILED RESULTS
# ============================================================================

results = {
    'model_path': MODEL_PATH,
    'num_classes': num_classes,
    'num_samples': len(true_classes),
    'overall_metrics': {
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1)
    },
    'per_class_metrics': {}
}

for i, name in enumerate(class_names):
    results['per_class_metrics'][name] = {
        'precision': float(precision_per_class[i]),
        'recall': float(recall_per_class[i]),
        'f1_score': float(f1_per_class[i]),
        'support': int(support_per_class[i])
    }

results_path = os.path.join(BASE_DIR, 'evaluation_results.json')
with open(results_path, 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print(f"‚úì Detailed results saved: {results_path}")

# ============================================================================
# FIND MISCLASSIFIED SAMPLES
# ============================================================================

print("\n" + "=" * 80)
print("PH√ÇN T√çCH L·ªñI")
print("=" * 80)

misclassified_indices = np.where(predicted_classes != true_classes)[0]
print(f"S·ªë m·∫´u b·ªã ph√¢n lo·∫°i sai: {len(misclassified_indices)} / {len(true_classes)}")
print(f"T·ª∑ l·ªá l·ªói: {len(misclassified_indices) / len(true_classes) * 100:.2f}%")

if len(misclassified_indices) > 0:
    print("\nM·ªôt s·ªë v√≠ d·ª• ph√¢n lo·∫°i sai:")
    for i, idx in enumerate(misclassified_indices[:10]):  # Hi·ªÉn th·ªã 10 l·ªói ƒë·∫ßu
        true_label = class_names[true_classes[idx]]
        pred_label = class_names[predicted_classes[idx]]
        confidence = predictions[idx][predicted_classes[idx]] * 100
        print(f"  {i+1}. Sample {idx}: True='{true_label}' ‚Üí Predicted='{pred_label}' (confidence: {confidence:.2f}%)")

# ============================================================================
# CONFIDENCE DISTRIBUTION
# ============================================================================

print("\n" + "=" * 80)
print("PH√ÇN B·ªê ƒê·ªò T·ª∞ TIN (CONFIDENCE)")
print("=" * 80)

max_confidences = np.max(predictions, axis=1)
correct_confidences = max_confidences[predicted_classes == true_classes]
incorrect_confidences = max_confidences[predicted_classes != true_classes]

print(f"ƒê·ªô t·ª± tin trung b√¨nh (ƒë√∫ng):  {np.mean(correct_confidences) * 100:.2f}%")
print(f"ƒê·ªô t·ª± tin trung b√¨nh (sai):   {np.mean(incorrect_confidences) * 100:.2f}%")
print(f"ƒê·ªô t·ª± tin cao nh·∫•t:           {np.max(max_confidences) * 100:.2f}%")
print(f"ƒê·ªô t·ª± tin th·∫•p nh·∫•t:          {np.min(max_confidences) * 100:.2f}%")

# Plot confidence distribution
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.hist(correct_confidences * 100, bins=20, color='green', alpha=0.7, label='Correct')
plt.hist(incorrect_confidences * 100, bins=20, color='red', alpha=0.7, label='Incorrect')
plt.xlabel('Confidence (%)')
plt.ylabel('Count')
plt.title('Confidence Distribution')
plt.legend()
plt.grid(axis='y', alpha=0.3)

plt.subplot(1, 2, 2)
plt.boxplot([correct_confidences * 100, incorrect_confidences * 100],
            labels=['Correct', 'Incorrect'])
plt.ylabel('Confidence (%)')
plt.title('Confidence Box Plot')
plt.grid(axis='y', alpha=0.3)

plt.tight_layout()
confidence_path = os.path.join(BASE_DIR, 'confidence_distribution.png')
plt.savefig(confidence_path, dpi=150, bbox_inches='tight')
print(f"‚úì Confidence distribution saved: {confidence_path}")

# ============================================================================
# SUMMARY
# ============================================================================

print("\n" + "=" * 80)
print("T·ªîNG K·∫æT")
print("=" * 80)
print(f"‚úì Model: {os.path.basename(MODEL_PATH)}")
print(f"‚úì Accuracy: {accuracy * 100:.2f}%")
print(f"‚úì Precision: {precision * 100:.2f}%")
print(f"‚úì Recall: {recall * 100:.2f}%")
print(f"‚úì F1-Score: {f1 * 100:.2f}%")
print(f"‚úì S·ªë ng∆∞·ªùi: {num_classes}")
print(f"‚úì T·ªïng s·ªë m·∫´u test: {len(true_classes)}")
print(f"‚úì Ph√¢n lo·∫°i ƒë√∫ng: {np.sum(predicted_classes == true_classes)}")
print(f"‚úì Ph√¢n lo·∫°i sai: {len(misclassified_indices)}")
print("\nFiles ƒë√£ t·∫°o:")
print(f"  1. {cm_path}")
print(f"  2. {metrics_path}")
print(f"  3. {confidence_path}")
print(f"  4. {results_path}")
print("=" * 80)

plt.show()

print("\nüéâ ƒê√ÅNH GI√Å HO√ÄN T·∫§T!")
